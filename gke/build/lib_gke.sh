#!/usr/bin/env bash

# Build GKE.
#
# This script is meant to be used on a traditional "workstation", or by being
# executed in the cloud using GCB. The main requirement is that the 'docker'
# binary is available on the PATH, as well as 'gcloud' and 'gsutil' (part of
# Google Cloud SDK).
#
# If this script runs on a workstation, then the build artifacts are left in the
# k/k repo. If this script detects that it is being run inside GCB, then build
# artifacts are uploaded to GCS.
#
# Almost all of the workings of this script is actually driven by a
# configuration file that is read in at the beginning, with the
# GKE_BUILD_CONFIG=... argument.

# Be strict.
set -o errexit
set -o nounset
set -o pipefail

SCRIPT_DIR="$(dirname "$(realpath "$0")")"
KUBE_ROOT="$(cd "${SCRIPT_DIR}" && git rev-parse --show-toplevel)"

# shellcheck source=./lib_assert.sh
source "${SCRIPT_DIR}"/lib_assert.sh

# shellcheck source=./lib_log.sh
source "${SCRIPT_DIR}"/lib_log.sh

# shellcheck source=./lib_yaml.sh
source "${SCRIPT_DIR}"/lib_yaml.sh # for _yq()

gke_build_entrypoint()
{
  local compile_done=0
  if (( $# == 0 )); then
    cat <<EOF >&2
usage: $0 [KEY=VALUE..]

  Note on arguments
  -----------------

  All arguments follow the format \`KEY=VALUE', where both KEY and VALUE are
  strings. The order of arguments do not matter.

  Required arguments
  ------------------

  GKE_BUILD_ACTIONS must be a CSV string with one or more of the following
  values:
    a. compile
    b. package
    c. validate
    d. push-gcs
    e. push-gcr
    f. clean        (deletes build outputs to free up disk space)
    g. print-version (print the version string to be used for the build)

  E.g., \`GKE_BUILD_ACTIONS=compile,package,validate' will run the compilation,
  packaging, and validation steps.

  Example invocation with only the required arguments to compile and package:

    $ $0 GKE_BUILD_ACTIONS=compile,package

  Optional arguments
  ------------------

  The following are optional arguments:

  - GKE_BUILD_CONFIG
  - VERSION
  - VERSION_SUFFIX
  - INJECT_DEV_VERSION_MARKER
  - TARGET_PLATFORMS
  - GCR_REPO
  - GCS_BUCKET
  - SKIP_DOCKER
  - TEMP_DIR

  [GKE_BUILD_CONFIG] is the path to a YAML file with all settings related to the
  build. If it is not supplied, the default "dev" settings are used, which are
  located in \`./config/common.yaml' and \`./config/louhi_test.yaml'.

  If this string is a CSV, then each config is used as an additive layer
  on top of the previous, such that the very last YAML specified in the
  CSV is the overall "winner" for any configuration setting. This is
  useful if you want to layer your configs on top of each other, using
  the first YAML as a kind of "base" YAML.


  [VERSION] is the version string to use. It wholly overrides the default
  version string generated by the build system. It must start with a \`v', and
  will be used in its *entirety*. E.g., if you give \`vfoobar', then the string
  \`vfoobar' will be used as-is for the version string.

  When compiling, the version string is fed into the gitVersion field of
  binaries so that they report this string as part of their version information.

  During packaging, this is used for tagging all Docker images and also for
  setting the toplevel folder name to be pushed up to GCS (in a subsequent
  \`push-gcs' step).


  [VERSION_SUFFIX] is used for appending an arbitrary string to the end of the
  version string. If you provide a VERSION_SUFFIX=$USER (without setting
  VERSION), then the \`$USER' string will get appended to the default version
  string generated by the build.

  If both VERSION and VERSION_SUFFIX is set, then the final version string
  generated will be "VERSION-VERSION_SUFFIX" (where the two strings
  concatenanted by a dash).


  [INJECT_DEV_VERSION_MARKER] decides whether to inject a '-gke.99.99+' string
  into the version string. Can be '1' or '0'. Default is '0' (false).


  [TARGET_PLATFORMS] is used to override "build-env.target-platforms" given in
  GKE_BUILD_CONFIG. It specifies the list of target platforms for all build
  actions, for example, "linux/amd64,windows/amd64"


  [GCR_REPO] (optional) is the name prefix for all Docker images during
  packaging. If specified, this overrides the "package.gcr.repo" field in
  the GKE_BUILD_CONFIG.


  [GCS_BUCKET] (optional) is the name of the GCS bucket to push up the GCS
  contents into. For example, if a version "v1.2.3" is created such that there
  are GCS artifacts in a folder named "v1.2.3", then setting GCS_BUCKET to
  gs://foo will upload "v1.2.3" into this bucket, such that eventually
  gs://foo/v1.2.3 is pushed up. By default this is set to 'push.gcs.to' field in
  GKE_BUILD_CONFIG.


  [SKIP_DOCKER] whether to skip generation of Docker images during the 'package'
  step in GKE_BUILD_ACTIONS. Can be '1' or '0'. Default is '0' (false).


  [TEMP_DIR] specifies a different temporary directory to use for builds.
  Default is /tmp.


  Examples
  --------

  See gke/build/README.md.
EOF
    return 1
  fi

  # Explicitly set the umask, to prevent users umask from leaking through.
  # This can cause permission denied errors if images are run as non-root.
  umask 022

  # Run self-test to ensure that some invariants are observed.
  self_test
  bootstrap_tooling

  process_args "$@"
  set_global_vars

  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*print-version.* ]]; then
    echo "${KUBE_GIT_VERSION}"
    return 0
  fi
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*enter-build-container.* ]]; then
    prepare_build_env
    docker run --rm \
      -e KUBE_ROOT="${__KUBE_ROOT_MOUNT_PATH}" \
      -e KUBE_OUTPUT_SUBPATH="${__output_subpath}" \
      -v "${KUBE_ROOT}":"${__KUBE_ROOT_MOUNT_PATH}" \
      -w "${__KUBE_ROOT_MOUNT_PATH}" \
      --entrypoint=bash \
      -it \
      "${__compiler_image_full}"
    return 0
  fi
  # The compilation stage is where the "VERSION" of the binaries are set. If
  # we want to set the VERSION, we must do it here.
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*compile.* ]]; then
    log_header "COMPILE"
    prepare_build_env push
    compile
    compile_done=1
    log_header "COMPILE OK"
  fi
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*package.* ]]; then
    log_header "PACKAGE"
    # If we're asked to "package" in isolation, we have to call
    # prepare_build_env because packaging involves source code injection that
    # that must happen from inside the compiler image.
    if ! ((compile_done)); then
      prepare_build_env push
    fi
    package
    log_header "PACKAGE OK"
  fi
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*validate.* ]]; then
    log_header "VALIDATE"
    validate
    log_header "VALIDATE OK"
  fi
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*push-gcs.* ]]; then
    log_header "PUSH GCS"
    push_gcs
    log_header "PUSH GCS OK"
  fi
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*push-gcr.* ]]; then
    log_header "PUSH GCR"
    push_gcr
    log_header "PUSH GCR OK"
  fi
  if [[ "${__GKE_BUILD_ACTIONS}" =~ .*clean.* ]]; then
    log_header "CLEAN"
    clean
    log_header "CLEAN OK"
  fi
  echo "ALL DONE"
}

# Process all arguments that are given on the command line, and set up a few
# global variables. These globals are then reconciled against the defaults in
# GKE_BUILD_CONFIG in set_global_vars.
process_args()
{
  for arg in "$@"; do
    case "${arg}" in
    GKE_BUILD_CONFIG=*)
      __GKE_BUILD_CONFIG="${arg#GKE_BUILD_CONFIG=}"
      ;;
    GKE_BUILD_ACTIONS=*)
      __GKE_BUILD_ACTIONS="${arg#GKE_BUILD_ACTIONS=}"
      ;;
    VERSION=*)
      # This is the version to thread into the binaries and also the
      # Docker images. If not set, then we just use the default underlying
      # version, which is auto-generated with `git`.
      __VERSION="${arg#VERSION=}"
      ;;
    VERSION_SUFFIX=*)
      # A suffix to append to the end of the version. This is useful if you want
      # to auto-generate the version, but still want to append some static
      # string to the of the version string. A typical use case is
      # `VERSION_SUFFIX=${USER}`, so that `-$USER` gets appended (the dash
      # separator is inserted automatically).
      __VERSION_SUFFIX="${arg#VERSION_SUFFIX=}"
      ;;
    INJECT_DEV_VERSION_MARKER=*)
      # Whether to invoke inject_gke_dev_version_marker().
      __INJECT_DEV_VERSION_MARKER="${arg#INJECT_DEV_VERSION_MARKER=}"
      ;;
    ASSERT_PROD_VERSION=*)
      # Abort the build if the version that will be used is not a
      # prod-compatible version.
      __ASSERT_PROD_VERSION="${arg#ASSERT_PROD_VERSION=}"
      ;;
    TARGET_PLATFORMS=*)
      __TARGET_PLATFORMS_STR="${arg#TARGET_PLATFORMS=}"
      ;;
    GCR_REPO=*)
      # For purposes of this script, this is the registry where we want to push
      # our images. It is also used to set KUBE_DOCKER_REGISTRY. This
      # KUBE_DOCKER_REGISTRY is used during the `package` step to name each
      # image's repository. When the `push-gcr` step is run afterwards, it will
      # use this name to find all images that need to be pushed up, and push
      # them up to GCR.
      __GCR_REPO="${arg#GCR_REPO=}"
      ;;
    GCS_BUCKET=*)
      __GCS_BUCKET="${arg#GCS_BUCKET=}"
      ;;
    SKIP_DOCKER=*)
      # Accepts either 0 (false) or 1 (true) as a value.
      #
      # Allows skipping Docker image generation (into the local Docker daemon)
      # during the packaging step. Useful for debugging tarball generation only.
      __SKIP_DOCKER="${arg#SKIP_DOCKER=}"
      ;;
    SKIP_DOCKER_LICENSE_INJECTION=*)
      # Accepts either 0 (false) or 1 (true) as a value.
      #
      # When building Docker images, skip license injection steps (if any).
      __SKIP_DOCKER_LICENSE_INJECTION="${arg#SKIP_DOCKER_LICENSE_INJECTION=}"
      ;;
    SKIP_DOCKER_SOURCE_INJECTION=*)
      # Accepts either 0 (false) or 1 (true) as a value.
      #
      # When building Docker images, skip source injection steps (if any).
      __SKIP_DOCKER_SOURCE_INJECTION="${arg#SKIP_DOCKER_SOURCE_INJECTION=}"
      ;;
    TEMP_DIR=*)
      # Specify a temporary working directory to actually store the build
      # artifacts. Useful if your /tmp directory (default choice for mktemp) is
      # mounted in a nearly-full partition.
      export TMPDIR="${arg#TEMP_DIR=}"
      ;;
    ENTER_BUILD_CONTAINER=*)
      # Invoke Docker to enter the build container. This is a convenience
      # wrapper for debugging build issues. Unlike SKIP_DOCKER=0, we don't
      # preserve the existing "${__GKE_BUILD_ACTIONS}" variable because entering
      # the build container takes precedence over everything else.
      __ENTER_BUILD_CONTAINER="${arg#ENTER_BUILD_CONTAINER=}"
      if [[ "${__ENTER_BUILD_CONTAINER:-}" == 1 ]]; then
        __GKE_BUILD_ACTIONS="enter-build-container"
      fi
      ;;
    *) log.fail "unrecognized argument \`${arg}'"
      ;;
    esac
  done

  if [[ -z "${__GKE_BUILD_CONFIG:-}" ]]; then
    log.fail "argument \`GKE_BUILD_CONFIG=...' not supplied"
  fi
  log.debugvar __GKE_BUILD_CONFIG

  # Additional processing for __GKE_BUILD_CONFIG to take care of CSVs.
  # If it is a CSV, we populate a new internal variable
  # __GKE_BUILD_CONFIGS, such that it will get picked up by get_val().
  if [[ "${__GKE_BUILD_CONFIG:-}" == *,* ]]; then
    IFS=',' read -ra __GKE_BUILD_CONFIGS <<< "${__GKE_BUILD_CONFIG}"
    log.debugvar __GKE_BUILD_CONFIGS
  fi

  if [[ -z "${__GKE_BUILD_ACTIONS:-}" ]]; then
    log.fail "argument \`GKE_BUILD_ACTIONS=...' must be supplied"
  fi

  # Set platforms.
  __TARGET_PLATFORMS=()
  if [[ -n "${__TARGET_PLATFORMS_STR:-}" ]]; then
    if [[ "${__TARGET_PLATFORMS_STR:-}" == *,* ]]; then
      IFS=',' read -ra __TARGET_PLATFORMS <<< "${__TARGET_PLATFORMS_STR}"
    else
      __TARGET_PLATFORMS=("${__TARGET_PLATFORMS_STR}")
    fi
  fi
  log.debugvar __TARGET_PLATFORMS

  # If we explicitly set "SKIP_DOCKER=0", check that we also wanted to run the
  # `package` step. If not, the two options are incompatible, so error out to
  # correct the mistake of the caller.
  if [[ "${__SKIP_DOCKER:-}" == 0 ]]; then
    if [[ ! "${__GKE_BUILD_ACTIONS}" =~ .*package.* ]]; then
      log.fail "
  \`SKIP_DOCKER=0' provided, but \`package' is not provided to
  GKE_BUILD_ACTIONS. Either remove \`SKIP_DOCKER=0', or additionally provide
  \`package' to GKE_BUILD_ACTIONS."
    fi
  fi
}

# Inject "-gke.99.99\+" as a substring if there is no "-gke.n.n+" substring
# found. This is to make sure that other downstream systems can recognize it as
# a GKE build for dev/test environments. See
# go/gke-build-k8s#test-out-your-change.
#
# If there is already a "-gke.N..." found, we preserve this "N" so that no
# information is lost.
inject_gke_dev_version_marker()
{
  local version="${1}"
  local new_version
  local extra_part

  log.info "looking at \`${version}'"

  # If the version already has a "-gke.N.N+" substring, then keep it as-is
  # (NOP).
  if [[ "${version}" =~ -gke\.[[:digit:]]+\.[[:digit:]]+\+ ]]; then
    echo "${version}"
    log.info "\`-gke.N.N+' already found (${BASH_REMATCH[0]}); NOP result"
    return
  fi

  # The version string does not look like "-gke.N.N+...". If we already have
  # some information we can preserve, namely the first `N' in what would
  # ultimately become "-gke.N.N+...",
  # keep it. Examples:
  #
  #   - Dev build exactly on a GKE prod tag (v1.20.0-gke.1200).
  #   "v1.20.0-gke.1200+0-g<HASH>" => "v1.20.0-gke.1200.99+0-g<HASH>"
  #   "v1.20.2-gke.800-1-g8783f10452667a" => "v1.20.2-gke.800.99+1-g8783f10452667a"
  if [[ "${version}" =~ ^(.+)(-gke\.[[:digit:]]+)(.*)$ ]]; then
    extra_part="${BASH_REMATCH[3]}"
    new_version="${BASH_REMATCH[1]}${BASH_REMATCH[2]}.99"
    if [[ -n "${extra_part:-}" ]]; then
      # Remove any leading - or + from extra_part.
      extra_part="${extra_part#-}"
      extra_part="${extra_part#+}"
      new_version="${new_version}+${extra_part}"
      log.info "\`-gke.N' found (${BASH_REMATCH[2]}) with extra_part ${extra_part}; preserving as \`${new_version}'"
      echo "${new_version}"
      return
    else
      # If there is no extra part at the end of a gke version number wever, .g.,
      # we have just "v1.20.0-gke.1200" to work with), then we would like to
      # append just ".99", not ".99+" as this leaves a dangling "+" sign.
      # However,
      # https://source.corp.google.com/piper///depot/google3/cloud/kubernetes/engine/common/version.go;l=160
      # expects there to be some text with a "+dev-unknown" at the end to denote
      # a dev version, so we have to construct a string on our own.
      new_version="${new_version}+dev-unknown"
      log.info "\`-gke.N' found (${BASH_REMATCH[2]}); preserving as \`${new_version}'"
      echo "${new_version}"
      return
    fi
  fi

  # Otherwise, there is nothing to preserve so inject "-gke.99.99+" as soon as
  # we find a '-' character. This matches the old behavior where we used to just
  # pipe to sed like this:
  #
  #   sed '/-gke\.[0-9]\+\.[0-9]\++/! s/-/-gke.99.99+/'
  #
  # Example:
  #
  #   "v1.21.0-alpha.0-692-g94a623a45a77eb" => "v1.21.0-gke.99.99+alpha.0-692-g94a623a45a77eb"
  if [[ "${version}" =~ ^[^-]+- ]]; then
    # Inject the substring into the middle of the string.
    new_version="${version/-/-gke.99.99+}"
    log.info "nothing to preserve; injecting \`-gke.99.99+' directly as \`${new_version}'"
    echo "${new_version}"
    return
  else
    # Append the substring at the end. However we also need to prevent appending
    # a trailing '+' at the end, so we inject '+dev-unknown' at the end instead
    # of just '+'.
    new_version="${version}-gke.99.99+dev-unknown"
    log.info "nothing to preserve; appending \`-gke.99.99+dev-unknown' at the end as \`${new_version}'"
    echo "${new_version}"
    return
  fi
}

# Checks whether the given string is a valid Docker tag. Returns 1 or 0.
is_valid_docker_tag()
{
  local tag="${1}"

  # From https://docs.docker.com/engine/reference/commandline/tag/.
  #
  # A tag name must be valid ASCII and may contain lowercase and uppercase
  # letters, digits, underscores, periods and dashes. A tag name may not start
  # with a period or a dash and may contain a maximum of 128 characters.

  if [[ "${tag}" =~ [^0-9A-Za-z._-] ]]; then
    log.warn "Docker tag \`${tag}' has an invalid character: ${BASH_REMATCH[*]}"
    return 1
  fi

  if [[ "${tag}" =~ ^[.-] ]]; then
    log.warn "Docker tag \`${tag}' is invalid because it starts with a period or dash"
    return 1
  fi

  if (( ${#tag} > 128 )); then
    log.warn "Docker tag \`${tag}' (${#tag} characters) is invalid because it is longer than 128 characters"
    return 1
  fi

  if [[ -z "${tag}" ]]; then
    log.warn "Docker tag \`${tag}' is invalid because it is empty"
    return 1
  fi
}

# Considers whether a version can be cast safely to a valid docker tag, using a
# traditional one-time '+' character replacement with '_'.
version_can_traditionally_cast_to_valid_docker_tag()
{
  local version="${1}"

  is_valid_docker_tag "$(cast_traditionally_to_pseudo_valid_docker_tag "${version}")"
}

# Replace a single "+" with a "_". This logic pervades the OSS build system and
# is something that has trickled down to other systems, and is something we have
# to respect where possible.
cast_traditionally_to_pseudo_valid_docker_tag()
{
  echo "${1/+/_}"
}

# Check if a given version string looks like either a GKE release (prod) or CI
# (dev) version string.
well_formed_version()
{
  local version="${1}"

  is_prod_version "${version}" || is_dev_version "${version}"
}

#
# Example of a GKE release (prod) version: "v1.18.14-gke.1700"
is_prod_version()
{
  local version="${1}"

  if [[ "${version}" =~ ^v[[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+-gke\.[[:digit:]]+$ ]]; then
    log.info "version \`${version}' is OK for a GKE release (prod)"
    return 0
  fi

  log.warn "version \`${version}' is NOT OK for a GKE release (prod expects vX.Y.Z-gke.N)"
  return 1
}

# Example of a CI/dev version:             "v1.11.10-gke.5.0+autoscaling-ci-1560135719-ca-938b6b2e6"
#
# Notice the "-gke.n.n+" pattern in the CI version (as well as the '+' sign).
is_dev_version()
{
  local version="${1}"

  if [[ "${version}" =~ ^v[[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+-gke\.[[:digit:]]+\.[[:digit:]]+\+[0-9a-zA-Z.-]+$ ]]; then
    log.info "version \`${version}' is OK for CI/dev"
    return 0
  fi

  log.warn "version \`${version}' is NOT OK for CI/dev (CI/dev expects vX.Y.Z-gke.N.N+...)"
  return 1
}

# Sets the version of all binaries when they are compiled. This is also used by
# Docker images when they are created.
set_version()
{
  local version="${1:-}"
  local version_suffix="${2:-}"

  case "${version}" in
  v*)
    KUBE_GIT_VERSION="${version}"
    ;;
  *)
    if [[ -n "${version:-}" ]]; then
      log.fail "VERSION must start with a leading 'v' character (got \`${version}')"
    fi

    # At this point, no VERSION has been specified, so we have to generate a
    # default one.

    # Set default version string by asking git.
    KUBE_GIT_VERSION=$(git describe --tags --match='v*' --abbrev=14)

    # If requested, inject a "-gke.99.99+" string if there isn't one like that
    # ("-gke.n.n+") already.
    if (( ${__INJECT_DEV_VERSION_MARKER:-0} )); then
      # If we are asked to inject a dev version marker, we *cannot* use a
      # version string that exactly matches a tag (prod version), such as
      # "v1.20.0-gke.1000" as-is as an argument to
      # inject_gke_dev_version_marker(). This is because we'll end up with the
      # string "v1.20.0-gke.1000.99+", which not only looks wrong but also hides
      # information about the exact commit hash. Even though the tag
      # "v1.20.0-gke.1000" is unique and we can derive the commit hash, it's
      # just good practice to have a commit hash next to every dev version
      # build, so we make sure that there is such commit hash information for
      # this case.
      #
      # This is why we re-generate the "${KUBE_GIT_VERSION}" again, but this
      # time force git to give us the commit hash.
      if is_prod_version "${KUBE_GIT_VERSION}"; then
        KUBE_GIT_VERSION=$(git describe --tags --match='v*' --abbrev=14 --long)
      fi
      KUBE_GIT_VERSION=$(inject_gke_dev_version_marker "${KUBE_GIT_VERSION}")
    fi

    if ! version_can_traditionally_cast_to_valid_docker_tag "${KUBE_GIT_VERSION}"; then
      log.warn "version \`${KUBE_GIT_VERSION}' cannot be cast (single replacement of '+' with '_') to a valid Docker tag; does it have more than 1 '+' or any other invalid Docker characters?"
      # Fail if we desire Docker images, but the version we're giving the system
      # cannot be traditionally cast to a valid Docker image tag.
      if ! (( ${__SKIP_DOCKER:-0} )); then
        log.fail "cannot produce Docker images with \$KUBE_GIT_VERSION \`${KUBE_GIT_VERSION}' (expected tag $(cast_traditionally_to_pseudo_valid_docker_tag "${KUBE_GIT_VERSION}") is invalid)"
      fi
    fi

    ;;
  esac

  # Append version suffix if it was set.
  if [[ -n "${version_suffix}" ]]; then
    log.info "appending version_suffix \`${version_suffix}' to the final version string"
    KUBE_GIT_VERSION="${KUBE_GIT_VERSION}-${version_suffix}"
  fi

  # Check that the version, after all the various options and such, is
  # well-formed by matching either a
  #
  #   (1) production-like form (e.g., "v1.18.14-gke.1700"), or
  #   (2) a CI/dev-like form (e.g., "v1.11.10-gke.5.0+autoscaling-ci-1560135719-ca-938b6b2e6").

  if ! well_formed_version "${KUBE_GIT_VERSION}"; then
    log.fail "\$KUBE_GIT_VERSION \`${KUBE_GIT_VERSION}' is not well-formed"
  fi

  log.debugvar KUBE_GIT_VERSION

  # Set Docker tag used with the KUBE_GIT_VERSION that we have.
  __docker_tag=$(cast_traditionally_to_pseudo_valid_docker_tag "${KUBE_GIT_VERSION}")
  if ! is_valid_docker_tag "${__docker_tag}"; then
    if (( ${__SKIP_DOCKER:-0} )); then
      log.warn "VERSION ${version} is not a valid Docker tag; ignoring because SKIP_DOCKER=1"
    else
      log.fail "VERSION ${version} is not a valid Docker tag; failing because SKIP_DOCKER=0"
    fi
  fi
  log.debugvar __docker_tag

  KUBE_GIT_MAJOR="$(echo "${KUBE_GIT_VERSION#v}" | cut -d. -f1)"
  KUBE_GIT_MINOR="$(echo "${KUBE_GIT_VERSION#v}" | cut -d. -f2)"

  # The KUBE_GIT_VERSION is used in
  #
  #   - make cross[-in-a-container]
  #   `- make all WHAT=...
  #     `- hack/make-rules/build.sh
  #     `- hack/lib/init.sh
  #       `- hack/lib/version.sh
  #     `- kube::golang::build_binaries
  #       `- kube::version::ldflags (sets $goldflags)
  #       `- kube::version::get_version_vars
  #         `- if KUBE_GIT_VERSION_FILE exists, use that, otherwise call `git` directly to retrieve `gitVersion` to stamp golang binaries
  #       `- kube::golang::build_binaries_for_platform ($build_args set with $goldflags)
  #       `- kube::golang::build_some_binaries ($build_args used with `go install`)
  #
  # to set the version information inside binaries during plain `go install`
  # compilation.
  #
  # The build/package-tarballs.sh also calls `kube::version::get_version_vars`
  # to set KUBE_GIT_VERSION, before using that for setting the Docker tag for
  # images in kube::release::create_docker_images_for_server..

  KUBE_GIT_COMMIT="$(git rev-parse HEAD'^{commit}')"

  if git_status=$(git status --porcelain 2>/dev/null) && [[ -z "${git_status}" ]]; then
    KUBE_GIT_TREE_STATE="clean"
  else
    KUBE_GIT_TREE_STATE="dirty"
  fi

  # The .dockerized-kube-version-defs file is not tracked in Git, so we can
  # write to it without fear of messing up git behavior. For simplicity, we
  # only set it to the basename; later when we run `make cross` and
  # `package-tarballs.sh` inside containers, we will make sure that this
  # file's parent directory is the directory of __KUBE_ROOT_MOUNT_PATH.
  export KUBE_GIT_VERSION_FILE=".dockerized-kube-version-defs"
  cat <<EOF >"${KUBE_ROOT}/${KUBE_GIT_VERSION_FILE}"
KUBE_GIT_COMMIT="${KUBE_GIT_COMMIT}"
KUBE_GIT_TREE_STATE="${KUBE_GIT_TREE_STATE}"
KUBE_GIT_VERSION="${KUBE_GIT_VERSION}"
KUBE_GIT_MAJOR="${KUBE_GIT_MAJOR}"
KUBE_GIT_MINOR="${KUBE_GIT_MINOR}"
EOF

  log.debugvar KUBE_GIT_VERSION_FILE
}

# Setting these globals makes it so that we don't have to repeatedly call 'yq'
# for them to extract them from the config yaml.
set_global_vars()
{
  local platform
  local docker_registry_from_config
  local bc_binaries_len
  local bc_assertions_len
  local i

  log.info "setting global variables"

  # Use custom version string (if given) instead of querying git.
  set_version "${__VERSION:-}" "${__VERSION_SUFFIX:-}"
  cat >&2 "${KUBE_ROOT}/${KUBE_GIT_VERSION_FILE}"

  if (( ${__ASSERT_PROD_VERSION:-0} )); then
    if is_prod_version "${KUBE_GIT_VERSION}"; then
      log.info "ASSERT_PROD_VERSION=1, and \$KUBE_GIT_VERSION \`${KUBE_GIT_VERSION}' is compatible with production"
    else
      log.fail "ASSERT_PROD_VERSION=1, but \$KUBE_GIT_VERSION \`${KUBE_GIT_VERSION}' is not compatible with production"
    fi
  fi

  # Set the Docker registry. The main significance of this is during
  # package-tarballs.sh, when the Docker images are named and tagged with this
  # registry.
  docker_registry_from_config=$(get_val 'package.gcr.repo')
  export KUBE_DOCKER_REGISTRY="${docker_registry_from_config}"
  if [[ -n "${__GCR_REPO:-}" ]]; then
    export KUBE_DOCKER_REGISTRY="${__GCR_REPO}"
  fi

  # We actually need experimental support anyway when doing `docker manifest`
  # commands in push_gcr(), so this just enables it globally for all steps.
  export DOCKER_CLI_EXPERIMENTAL=enabled
  # Disable building the conformance image, which we don't need for GKE.
  export KUBE_BUILD_CONFORMANCE=n

  __for_gcs=$(get_val "package.gcs.to")
  # Remove and replace literal "${KUBE_ROOT}" with the actual value.
  __for_gcs=${__for_gcs/\${KUBE_ROOT\}/${KUBE_ROOT}}
  log.debugvar __for_gcs

  __target_platforms=()
  # If we are explicitly given platforms to build for on the command line, use
  # them.
  if (( ${#__TARGET_PLATFORMS[@]} )); then
    __target_platforms=("${__TARGET_PLATFORMS[@]}")
  else
    log.info "getting target platforms from configuration file"
    for ((i=0; i<$(get_val "build-env.target-platforms" -l); ++i)); do
      __target_platforms+=("$(get_val "build-env.target-platforms[$i]")")
    done
  fi
  log.debugvar __target_platforms

  if (( ${#__target_platforms} == 0 )); then
    log.fail "\`\$__target_platforms' cannot be empty"
  fi
  # Check that the given platforms are supported.
  for platform in ${__target_platforms[@]+"${__target_platforms[@]}"}; do
    case "${platform}" in
      linux/arm64) ;;
      linux/amd64) ;;
      windows/amd64) ;;
      *) log.fail "unsupported platform ${platform}" ;;
    esac
  done

  # KUBE_BUILD_PLATFORMS is expected by some OSS scripts that are run later on.
  # NOTE: These variables must be injected at the time we run the container to
  # build the correct platform (with the '-e' flag to 'docker run').
  export KUBE_BUILD_PLATFORMS="${__target_platforms[*]}"
  log.debugvar KUBE_BUILD_PLATFORMS

  # Unlike __for_gcs, we cannot replace the string "${KUBE_ROOT}" with the
  # actual value because ./copy-host-source.sh uses this, and that script is run
  # inside a container with a possibly different "${KUBE_ROOT}" layout.
  __output_subpath=$(get_val "compile.output-subpath")
  # Remove leading literal "${KUBE_ROOT}/" prefix (if any).
  __output_subpath="${__output_subpath#\${KUBE_ROOT\}/}"
  log.debugvar __output_subpath

  __tars_subpath=$(get_val "package.tars-subpath")
  # Remove leading literal "${KUBE_ROOT}/" prefix (if any).
  __tars_subpath="${__tars_subpath#\${KUBE_ROOT\}/}"
  log.debugvar __tars_subpath

  # This is the path within a container where will mount KUBE_ROOT into.
  __KUBE_ROOT_MOUNT_PATH=$(get_val "compile.kube-root-mount-path")
  export __KUBE_ROOT_MOUNT_PATH
  log.debugvar __KUBE_ROOT_MOUNT_PATH

  # Set variables for boringcrypto-built binaries. This is done during
  # validation.
  __golang_boringcrypto_image=$(get_val "build-env.compiler-image.deps.golang-boringcrypto-image")
  log.debugvar __golang_boringcrypto_image
  if [[ -n "${__golang_boringcrypto_image}" ]]; then
    __boringcrypto_bins=""
    bc_binaries_len="$(get_val "validate.boringcrypto.binaries" -l)"
    for ((i=0; i<bc_binaries_len; ++i)); do
      __boringcrypto_bins+="$(get_val "validate.boringcrypto.binaries[$i]")",
    done
    # Drop trailing comma.
    __boringcrypto_bins="${__boringcrypto_bins:0: -1}"
    log.debugvar __boringcrypto_bins

    # Make a space-separated list of bins that should be compiled with CGO enabled.
    # See `KUBE_CGO_OVERRIDES` in hack/lib/golang.sh.
    # Boringcrypto requires cgo to be enabled.
    __KUBE_CGO_OVERRIDES="$(sed "s/,/ /g" <<< "${__boringcrypto_bins}")"
    log.debugvar __KUBE_CGO_OVERRIDES
    # Set the `gkeboringcrypto` build flag.
    # Note: as of release `b6`, the go-boringcrypto buildchain automatically
    # sets the `boringcrypto` build tag.
    __GOFLAGS='-tags=gkeboringcrypto'
  fi

  # This overrides the OSS `go-runner` runtime base image.
  __go_runner_image=$(get_val "build-env.runtime-image.go-runner")
  log.debugvar __go_runner_image

  # This overrides the OSS `kube-proxy` runtime base image.
  __kube_proxy_base_image=$(get_val "build-env.runtime-image.kube-proxy-base")
  log.debugvar __kube_proxy_base_image

  # This overrides the OSS `setcap` runtime image.
  __setcap_image=$(get_val "build-env.runtime-image.setcap")
  log.debugvar __setcap_image

  # Log remaining variables of interest.
  log.debugvar __INJECT_DEV_VERSION_MARKER
  log.debugvar __SKIP_DOCKER
  log.debugvar __GCR_REPO
  log.debugvar __GCS_BUCKET
  log.debugvar TMPDIR
  log.debugvar __GKE_BUILD_ACTIONS
}

# Dynamically creates a build env image tag, from the various inputs provided.
set_compiler_image_tag()
{
  local etcd_version
  local protobuf_version
  local golang_image_full
  local golang_tag

  etcd_version=$(get_val "build-env.compiler-image.deps.etcd")
  protobuf_version=$(get_val "build-env.compiler-image.deps.protobuf")
  golang_image_full=$(get_val "build-env.compiler-image.deps.golang-image")
  if [[ -n "${__golang_boringcrypto_image}" ]]; then
    golang_image_full="${__golang_boringcrypto_image}"
  fi
  golang_image_only="${golang_image_full%:*}"
  golang_image_only="${golang_image_only##*/}"
  golang_tag="${golang_image_full#*:}"
  __compiler_image_tag="${golang_image_only}-${golang_tag}-etcd-${etcd_version}-protobuf-${protobuf_version}"
  if (( ${#__compiler_image_tag} > 128 )); then
    # Docker tags may not exceed 128 characters.
    log.warn "__compiler_image_tag \`${__compiler_image_tag}' is over 128 characters; truncating to 128"
    __compiler_image_tag="${__compiler_image_tag:0:128}"
    log.debugvar __compiler_image_tag
  fi
}

# Install dependencies that this script itself depends on.
bootstrap_tooling()
{
  # Currently we just need to install the `yq' binary. It is not fatal if the
  # installation fails, because we can fall back to a Docker container of it. We
  # first try to see if there's already a `yq' binary in our PATH, and if so,
  # check the version. If neither condition holds, we install our own version
  # into "${tools_dir}"/bin and modify PATH to use it downstream.

  # Set installation path.
  local tools_dir="${SCRIPT_DIR}/tools/bin"
  local yq_version="3.4.1"

  # If there's already a version of yq available *somewhere* in the PATH, and it
  # is at the correct version, don't install it.
  if 2>/dev/null >&2 command -v yq; then
    if [[ "$(yq --version)" == "yq version ${yq_version}" ]]; then
      log.info "yq ${yq_version} already exists at $(type -p yq)"
      return
    fi
  fi

  # Check if there is already a version of yq in the "${tools_dir}"/yq path.
  if [[ -x "${tools_dir}"/yq ]]; then
    if [[ "$("${tools_dir}"/yq --version)" == "yq version ${yq_version}" ]]; then
      log.info "yq ${yq_version} already exists at ${tools_dir}/yq"
      # Export this path so that we can just refer to it as `yq' from now on.
      export PATH="${tools_dir}:${PATH}"
      return
    fi

    # Because we have the wrong version of yq, delete it.
    rm -f "${tools_dir}"/yq
  fi

  # Install yq into ${SCRIPT_DIR}/bin.
  if GOBIN="${tools_dir}" \
    go install github.com/mikefarah/yq/v3@"${yq_version}"; then

    # Modify $PATH to prefer our explicit version of yq over all others. This way
    # we don't have to check whether there is already a yq version (installed by
    # some other process) and what version it is.
    export PATH="${tools_dir}:${PATH}"
  else
    # It's OK if the installation fails, because by default we already fall back
    # to a containerized invocation of it.
    log.warn "yq installation failed"
  fi
}

# Prepare a build environment.
#
# OUTPUT: A Docker image that is present on the local Docker daemon, which we
# can mount and use to build Kubernetes inside.
prepare_build_env()
{
  local action="${1:-}"
  __compiler_image_name=$(get_val "build-env.compiler-image.name")
  log.assertvar __compiler_image_name

  set_compiler_image_tag
  __compiler_image_full="${__compiler_image_name}:${__compiler_image_tag}"

  # First try to use the image on disk (if it exists).
  if find_images "${__compiler_image_name}" "${__compiler_image_tag}"; then
    log.info "prepare_build_env: \`${__compiler_image_full}' found; using it for compilation"
    return
  fi

  # If the image is not on disk, try pulling from the GCR cache.
  if docker pull "${__compiler_image_full}"; then
    log.info "prepare_build_env: \`${__compiler_image_full}' pulled; using it for compilation"
    return
  fi

  log.info "\`${__compiler_image_full}' not found locally or in GCR; building"

  # Build the image locally from scratch.
  local etcd_version
  local golang_image_full
  local protobuf_version

  etcd_version=$(get_val "build-env.compiler-image.deps.etcd")
  protobuf_version=$(get_val "build-env.compiler-image.deps.protobuf")

  golang_image_full=$(get_val "build-env.compiler-image.deps.golang-image")
  if [[ -n "${__golang_boringcrypto_image}" ]]; then
    golang_image_full="${__golang_boringcrypto_image}"
  fi

  log.debugvar __compiler_image_full

  docker build \
    --build-arg ETCD_VERSION="${etcd_version}" \
    --build-arg GOLANG_IMAGE="${golang_image_full}" \
    --build-arg PROTOBUF_VERSION="${protobuf_version}" \
    -t "${__compiler_image_full}" \
    "${KUBE_ROOT}"/gke/build/cross/

  case "${action}" in
    push)
      # Push the image into the cache. It's OK to fail to push, as this is not
      # necessary for the rest of the build.
      docker push "${__compiler_image_full}" || true
      ;;
    *)
      if [[ -n "${action}" ]]; then
        log.fail "unsupported action ${action}"
      fi
      ;;
  esac
}

boringcrypto_hook()
{
  log.info "copying host sources"

  # NOTE: This script must be run from within the compiler image, because we
  # want to copy the source code from the compiler image's filesystem. This is
  # because if we compile with boringcrypto, we have to include the host's
  # source code. In this case, the "host" is the compiler image used to compile
  # kubernetes.
  docker run --rm \
    -e KUBE_ROOT="${__KUBE_ROOT_MOUNT_PATH}" \
    -e KUBE_OUTPUT_SUBPATH="${__output_subpath}" \
    -v "${KUBE_ROOT}":"${__KUBE_ROOT_MOUNT_PATH}" \
    -w "${__KUBE_ROOT_MOUNT_PATH}" \
    "${__compiler_image_full}" \
    gke/build/copy-host-source.sh

  # Inject licenses and source code into the appropriate vanilla tarballs for
  # compliance. It's important that we do this *before* calling arrange_for_gcs,
  # because that function treats everything generated up to that point as golden
  # (and also checksums them). IOW, modifying any of the tarballs *after*
  # calling arrange_for_gcs will break the checksums.

  # Inject LICENSES/host into all node tarballs.
  log.info "injecting LICENSES/host to all node tarballs"
  for platform in "${__target_platforms[@]}"; do
    tar_inject_paths \
      "${KUBE_ROOT}/${__tars_subpath}/kubernetes-node-${platform//\//-}.tar.gz" \
      "${KUBE_ROOT}/${__output_subpath}/src/LICENSES" \
      "host" \
      "kubernetes/LICENSES"
  done

  log.info "injecting LICENSES/host to all server tarballs"
  # Inject LICENSES/host into all server tarballs. Currently only linux
  # has server artifacts, so that's the only one we consider here.
  for platform in "${__target_platforms[@]}"; do
    local os="${platform%%/*}"
    case "${os}" in
        linux)
          tar_inject_paths \
            "${KUBE_ROOT}/${__tars_subpath}/kubernetes-server-${platform//\//-}.tar.gz" \
            "${KUBE_ROOT}/${__output_subpath}/src/LICENSES" \
            "host" \
            "kubernetes/LICENSES"
        ;;
    esac
  done

  # Inject LICENSES/host to the platform-independent tarball.
  log.info "injecting LICENSES/host to kubernetes.tar.gz"
  tar_inject_paths \
    "${KUBE_ROOT}/${__tars_subpath}/kubernetes.tar.gz" \
    "${KUBE_ROOT}/${__output_subpath}/src/LICENSES" \
    "host" \
    "kubernetes/LICENSES"

  # Inject the source code tarballs generated by /gke/build/copy-host-source.sh
  # into kubernetes-src.tar.gz. The source code tarballs are
  # "${__output_subpath}/src/*.tar".
  log.info "injecting dependency sources into kubernetes-src.tar.gz"
  tar_inject_tars \
    "${KUBE_ROOT}/${__tars_subpath}/kubernetes-src.tar.gz" "kubernetes" \
    "${KUBE_ROOT}/${__output_subpath}"/src/*.tar
}

compile()
{
  log.info "starting binary compilation"

  # Compile binaries. The version that the binaries report is set via
  # KUBE_GIT_VERSION_FILE.
  docker run --rm \
    -e KUBE_BUILD_PLATFORMS="${KUBE_BUILD_PLATFORMS}" \
    -e KUBE_OUTPUT_SUBPATH="${__output_subpath}" \
    -e KUBE_GIT_VERSION_FILE="${__KUBE_ROOT_MOUNT_PATH}/${KUBE_GIT_VERSION_FILE}" \
    -e KUBE_CGO_OVERRIDES="${__KUBE_CGO_OVERRIDES:-}" \
    -e GOFLAGS="${__GOFLAGS:-}" \
    -v "${KUBE_ROOT}":"${__KUBE_ROOT_MOUNT_PATH}" \
    -w "${__KUBE_ROOT_MOUNT_PATH}" \
    -u "$(id -u):$(id -g)" \
    "${__compiler_image_full}" \
    make cross-in-a-container

  # Print version (for debugging).
  docker run --rm \
    -v "${KUBE_ROOT}":"${__KUBE_ROOT_MOUNT_PATH}" \
    -w "${__KUBE_ROOT_MOUNT_PATH}" \
    "${__compiler_image_full}" \
    gke/cluster/kubectl.sh version --client

  log.info "binary artifacts are in ${KUBE_ROOT}/${__output_subpath}/bin"
}

# This is mainly for generating supporting GCS and GCR artifacts.
package()
{
  log.info "running package-tarballs.sh"

  # OSS package-tarballs.sh looks at relative folder paths from KUBE_ROOT, so we
  # have to position ourselves for it with pushd/popd.
  pushd "${KUBE_ROOT}"
  KUBE_OUTPUT_SUBPATH="${__output_subpath}" \
    KUBE_GIT_VERSION_FILE="${KUBE_ROOT}/${KUBE_GIT_VERSION_FILE}" \
    KUBE_GORUNNER_IMAGE="${__go_runner_image}" \
    KUBE_PROXY_BASE_IMAGE="${__kube_proxy_base_image}" \
    KUBE_BUILD_SETCAP_IMAGE="${__setcap_image}" \
    "${KUBE_ROOT}"/gke/build/package-tarballs.sh

  # For BoringCrypto, run associated hook for additional processing. This is
  # mainly about injecting the license and source code of dependencies into the
  # generated artifacts for compliance.
  if [[ -n "${__golang_boringcrypto_image:-}" ]]; then
    boringcrypto_hook
  fi

  if ! (( ${__SKIP_DOCKER:-0} )); then
    build_docker_images
  fi

  arrange_for_gcs
  popd
}

# Append into a tarball some paths, overwriting the original tarball. The very
# first argument is the tarball to augment. To give greater control over how the
# paths are actually added into the tarball, the user must specify 3 items for
# each path:
#
# 1) parent directory of the path
#
# 2) the rest of the path itself (this portion will appear in the tarball's `tar
# tf ...' output)
#
# 3) the prefix to use when appending this path into the tarball (optional, can
# be the empty string)
#
# Example usage:
#
#   tar_inject_paths foo.tar \
#     /some/other/dir \
#     foo/bar/baz \
#     quux/
#
#     => results in foo.tar appending the path "/some/other/dir/foo/bar/baz" as
#     "quux/foo/bar/baz" into the tarball.
#
tar_inject_paths()
{
  local orig_tarball="${1}"
  local orig_tarball_uncompressed="${orig_tarball%.gz}"
  shift
  declare -a args=("$@")
  local path_parent
  local path
  local tarball_prefix

  if (( "${#args[@]}" == 0 )); then
    log.fail "tar_inject_paths: need at least 4 arguments, with the tarball to append to as the first and the rest of the arguments in multiples of 3 (in the form <directory> <relative path from directory> <tarball_prefix_to_use> ...)"
  fi

  if (( "${#args[@]}" % 3 != 0 )); then
    log.fail "tar_inject_paths: need at least 4 arguments, with the tarball to append to as the first and the rest of the arguments in multiples of 3 (in the form <directory> <relative path from directory> <tarball_prefix_to_use> ...)"
  fi

  log.info "injecting into ${orig_tarball}"

  assert_path_exists "${orig_tarball}"

  # Lightly modify the original. We don't fully extract the tar. Instead, we
  # just gunzip it and then append into the tar archive directly.
  gunzip "${orig_tarball}"

  # Append paths into orig_tarball, with the right prefixes (if any).
  for ((i=0; i<"${#args[@]}"; i+=3)); do
    path_parent="${args[$i]}"
    path="${args[$((i + 1))]}"
    tarball_prefix="${args[$((i + 2))]}"
    tarball_prefix="${tarball_prefix%%/}"
    pushd "${path_parent}"
    if [[ -n "${tarball_prefix}" ]]; then
      tar rf "${orig_tarball_uncompressed}" \
        --xform "s|^|${tarball_prefix}/|" \
        "${path}"
    else
      tar rf "${orig_tarball_uncompressed}" "${path}"
    fi
    popd
  done

  # Compress it back up.
  gzip "${orig_tarball_uncompressed}"
}

# Like tar_inject_paths(), but (1) we only accept tarballs as paths to inject and (2)
# the resulting tarball's contents are all under one or more subdirectories so
# that we can differentiate the original's contents against what has been added.
# All of the original tarball's contents are prefixed with "${2}", so that the
# final listing (if you were to run `tar tf ${tarball}') would looks like this:
#
#   /${2}/foo/...
#   /${2}/bar/...
#   /path1_basename/path1/contents/...
#   /path1_basename/path1/contents/...
#   /path2_basename/path2/contents/...
#   /path3_basename/path3/contents/...
#
# The other paths that are appended have their prefix (subdirectory name) named
# after the path's basename.
#
# NOTE: There is no check made to ensure that subdirectory names are not shared.
# E.g., if you provide "foo.tar" and "another/path/to/foo.tar" as paths, their
# contents will both get added underneath a "/foo" directory.
#
# For comparison, if you used tar_inject_paths() instead of this function, you would
# get:
#
#   /foo/...
#   /bar/...
#   /path1/contents/...
#   /path1/contents/...
#   /path2/contents/...
#   /path3/contents/...
tar_inject_tars()
{
  local orig_tarball="${1}"
  local orig_tarball_uncompressed="${orig_tarball%.gz}"
  local orig_tarball_subdir="${2%%/}"
  shift 2
  declare -a tars=("$@")
  local workspace
  local workspace_subdir

  local tar_basename

  log.info "injecting into ${orig_tarball}"

  assert_path_exists "${orig_tarball}"

  workspace="$(mktemp -d -t gke-build-tar-inject.XXXXXXXXXX)"
  workspace_subdir="${workspace}/${orig_tarball_subdir}"
  mkdir -p "${workspace_subdir}"

  # Extract original tarball into its own subdir. This subdir will be present in
  # the resulting tar archive (visible with `tar tf ...'). The "--xform ..."
  # here removes any leading "./" from the resulting re-archived (overwritten)
  # tarball.
  gunzip "${orig_tarball}"
  tar xf "${orig_tarball_uncompressed}" -C "${workspace_subdir}"
  pushd "${workspace}"
  tar cf "${orig_tarball_uncompressed}" \
    --xform "s|^\./||" \
    ./*
  popd

  # Append tars into the tempfile. For each tarball, use its basename as the
  # subdir into the tempfile.
  for tar in "${tars[@]}"; do
    tar_basename="${tar##*/}"
    tar_basename="${tar_basename%%.*}"
    tar Af "${orig_tarball_uncompressed}" \
    --xform "s|^|${tar_basename}/|" \
      "${tar}"
  done

  gzip "${orig_tarball_uncompressed}"
}

validate()
{
  validate_licenses
  validate_gke_cluster_config_files
  validate_bc_sources
  validate_bc_bins
}

# Validate that the correct GKE internal cluster config files/scripts are
# copied into the kubernetes-manifests.tar.gz tarball.
validate_gke_cluster_config_files() {
  local artifacts_root="${__for_gcs}/${KUBE_GIT_VERSION}"
  local artifact="${artifacts_root}/kubernetes-manifests.tar.gz"
  assert_pathregex_exists_in_tar "${artifact}" "^kubernetes/gci-trusty/gke-internal-configure-helper.sh"
}

# Validate that the binaries were compiled with Go-BoringCrypto, as implied
# by the presence of the sentinel ASCII string in
# gke/fips/boringcrypto/certifiably_boring.go
validate_bc_bins()
{
  local artifacts_root="${__for_gcs}/${KUBE_GIT_VERSION}"
  local bin_dir="${artifacts_root}/bin/linux/amd64"
  local sentinel_bc_string='This GKE-distributed binary expects to be utilizing boringcrypto'

  log.info "validating go-boringcrypto binaries"

  local bc_binaries_len="$(get_val "validate.boringcrypto.binaries" -l)"
  for ((i=0; i<bc_binaries_len; ++i)); do
    local bc_bin="${bin_dir}/$(get_val "validate.boringcrypto.binaries[$i]")"
    assert_path_exists "${bc_bin}"

    log.info "validating ${bc_bin}"
    grep -aq "${sentinel_bc_string}" "${bc_bin}" || log.fail "${bc_bin} could not be confirmed to be compiled w/ go-boringcrypto"
  done
}

validate_licenses()
{
  local platform
  local artifacts_root
  local got_docker_tag
  local expected_docker_tag
  local unique_sums
  local artifact

  artifacts_root="${__for_gcs}/${KUBE_GIT_VERSION}"

  validate_tarballs_existence "${artifacts_root}"
  validate_extras "${artifacts_root}"

  log.info "running license check"
  # Check that license directory exists. Just look at the tarballs and see if
  # the files we expect there to be exist.

  # Check for LICENSES/host folder.
  for platform in "${__target_platforms[@]}"; do
    artifact="${KUBE_ROOT}/${__tars_subpath}/kubernetes-node-${platform//\//-}.tar.gz"
    log.info "checking for kubernetes/LICENSES/host/ in ${artifact}"
    assert_pathregex_exists_in_tar "${artifact}" "^kubernetes/LICENSES/host/"
  done

  for platform in "${__target_platforms[@]}"; do
    # Split platform into os and arch. The platform string format is os/arch.
    local os="${platform%%/*}"
    case "${os}" in
      linux)
        artifact="${KUBE_ROOT}/${__tars_subpath}/kubernetes-server-${platform//\//-}.tar.gz"
        log.info "checking for kubernetes/LICENSES/host/ in ${artifact}"
        assert_pathregex_exists_in_tar "${artifact}" "^kubernetes/LICENSES/host/"
        ;;
    esac
  done
  artifact="${artifacts_root}/kubernetes.tar.gz"
  log.info "checking for kubernetes/LICENSES/host/ in ${artifact}"
  assert_pathregex_exists_in_tar "${artifact}" "^kubernetes/LICENSES/host/"

  # Validate binary paths. The paths we expect to exist depend on the platforms
  # that we requested to build. This information is in the configuration YAML.
  log.info "checking for expected binary paths"
  for platform in "${__target_platforms[@]}"; do
    # Split platform into os and arch. The platform string format is os/arch.
    local os="${platform%%/*}"
    case "${os}" in
      linux)
        validate_linux "${artifacts_root}" "${platform}"
        ;;
      windows)
        validate_windows "${artifacts_root}" "${platform}"
        ;;
      *) log.fail "unsupported platform ${platform}" ;;
    esac
  done
}

# If we compile with boringcrypto, then because it is a C library we must
# include glibc sources.
validate_bc_sources()
{
  local artifacts_root
  local artifact

  artifacts_root="${__for_gcs}/${KUBE_GIT_VERSION}"

  log.info "running sources check"
  # Check for glibc folder (required for boringcrypto).
  artifact="${artifacts_root}/kubernetes-src.tar.gz"
  log.info "checking for kubernetes/LICENSES/host/ in ${artifact}"
  assert_pathregex_exists_in_tar "${artifact}" "^glibc/"
}

# Ensure all necessary tarballs exist.
validate_tarballs_existence()
{
  local artifacts_root="${1}"

  log.info "checking for toplevel tarballs"
  assert_path_exists "${artifacts_root}/kubernetes.tar.gz"
  assert_path_exists "${artifacts_root}/kubernetes-manifests.tar.gz"
  assert_path_exists "${artifacts_root}/kubernetes-src.tar.gz"
  assert_path_exists "${artifacts_root}/kubernetes-test-portable.tar.gz"

  log.info "checking for per-platform tarbalss"
  for platform in "${__target_platforms[@]}"; do
    log.info "checking for ${platform} tarballs"
    # Split platform into os and arch. The platform string format is os/arch.
    local os="${platform%%/*}"
    local arch="${platform##*/}"
    assert_path_exists "${artifacts_root}/kubernetes-node-${os}-${arch}.tar.gz"
    assert_path_exists "${artifacts_root}/kubernetes-client-${os}-${arch}.tar.gz"
    if [[ "${os}" == "linux" ]]; then
      assert_path_exists "${artifacts_root}/kubernetes-server-${os}-${arch}.tar.gz"
    fi
    assert_path_exists "${artifacts_root}/kubernetes-test-${os}-${arch}.tar.gz"
  done
}

validate_extras()
{
  local artifacts_root="${1}"

  log.info "checking paths in extra/"
  assert_path_exists "${artifacts_root}/extra/gce/configure.sh"
  assert_path_exists "${artifacts_root}/extra/gce/master.yaml"
  assert_path_exists "${artifacts_root}/extra/gce/node.yaml"
  assert_path_exists "${artifacts_root}/extra/gce/shutdown.sh"
  assert_path_exists "${artifacts_root}/extra/gce/windows/common.psm1"
  assert_path_exists "${artifacts_root}/extra/gce/windows/configure.ps1"
  assert_path_exists "${artifacts_root}/extra/gce/windows/install-ssh.psm1"
  assert_path_exists "${artifacts_root}/extra/gce/windows/k8s-node-setup.psm1"
  assert_path_exists "${artifacts_root}/extra/gce/windows/user-profile.psm1"
}

validate_linux()
{
  local artifacts_root="${1}"
  local platform="${2}"
  declare -a docker_tag_paths

  # TODO: Move these assertions out to the config YAML.
  log.info "checking paths in ${artifacts_root}/bin/${platform}"
  assert_path_exists "${artifacts_root}/bin/${platform}/apiextensions-apiserver"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-aggregator"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-apiserver"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-apiserver.docker_tag"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-apiserver.tar"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-controller-manager"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-controller-manager.docker_tag"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-controller-manager.tar"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-proxy"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-proxy.docker_tag"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-proxy.tar"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-scheduler"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-scheduler.docker_tag"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-scheduler.tar"
  assert_path_exists "${artifacts_root}/bin/${platform}/kubeadm"
  assert_path_exists "${artifacts_root}/bin/${platform}/kubectl"
  assert_path_exists "${artifacts_root}/bin/${platform}/kubelet"
  assert_path_exists "${artifacts_root}/bin/${platform}/mounter"

  # Check that the docker tag matches KUBE_GIT_VERSION (modulo invalid Docker
  # tag characters.)
  log.info "checking that the docker tag matches \${KUBE_GIT_VERSION} modulo invalid Docker tag characters (${__docker_tag})"
  got_docker_tag="$(cat "${artifacts_root}/bin/${platform}/kube-apiserver.docker_tag")"
  expected_docker_tag="${__docker_tag}"
  assert_variable_equality "${got_docker_tag}" "${expected_docker_tag}"

  # Check that all docker tags match among themselves, by ensuring all docker
  # tag files have the same content (as a checksum).
  log.info "check that all docker tags are identical"
  docker_tag_paths=(
    "${artifacts_root}"/bin/"${platform}"/kube-apiserver.docker_tag
    "${artifacts_root}"/bin/"${platform}"/kube-controller-manager.docker_tag
    "${artifacts_root}"/bin/"${platform}"/kube-proxy.docker_tag
    "${artifacts_root}"/bin/"${platform}"/kube-scheduler.docker_tag
  )
  unique_sums=$(md5sum "${docker_tag_paths[@]}" | cut -d' ' -f1 | uniq | wc -l)
  if (( unique_sums > 1 )); then
    for docker_tag_path in "${docker_tag_paths[@]}"; do
      log.info "${docker_tag_path}: $(cat "${docker_tag_path}")"
    done
    log.fail "assertion failure: docker tags are not identical"
  fi
}

validate_windows()
{
  local artifacts_root="${1}"
  local platform="${2}"

  log.info "checking paths in ${artifacts_root}/bin/${platform}"
  assert_path_exists "${artifacts_root}/bin/${platform}/kubeadm.exe"
  assert_path_exists "${artifacts_root}/bin/${platform}/kubectl.exe"
  assert_path_exists "${artifacts_root}/bin/${platform}/kubelet.exe"
  assert_path_exists "${artifacts_root}/bin/${platform}/kube-proxy.exe"
}

build_docker_images()
{
  local FROM_image
  local image_shortname
  local tarball_path
  local action
  local total_images
  local total_steps
  local total_active_steps
  local i
  local j
  declare -a to_skip=()

  total_images="$(get_val "package.gcr.images" -l)"
  log.debugvar total_images
  # Process all images we would like to build.
  for ((i=0; i<total_images; ++i)); do
    to_skip=()
    image_shortname=$(get_val "package.gcr.images[$i].name")
    log.debug "Building Docker image(s) for \`${image_shortname}'"

    total_steps=$(get_val "package.gcr.images[$i].build-steps" -l)
    # If we want to skip certain steps, this affects the $total_steps value.

    # We need to figure out which steps to skip. Build up an array of step
    # numbers to skip over. Later on when we hit a number in the array, we skip
    # it.
    for ((j=0; j<total_steps; ++j)); do
      action=$(get_val "package.gcr.images[$i].build-steps[$j].action")
      if [[ "${action}" == "inject-licenses" ]]; then
        if (( ${__SKIP_DOCKER_LICENSE_INJECTION:-0} )); then
          log.info "skipping license injection because __SKIP_DOCKER_LICENSE_INJECTION is on"
          to_skip+=("$j")
        fi
      fi
      if [[ "${action}" == "inject-source-code" ]]; then
        if (( ${__SKIP_DOCKER_SOURCE_INJECTION:-0} )); then
          log.info "skipping source injection because __SKIP_DOCKER_SOURCE_INJECTION is on"
          to_skip+=("$j")
        fi
      fi
    done

    # Determine how many *actual* steps will be executed.
    total_active_steps=$((total_steps - ${#to_skip[@]}))
    log.info "total active steps: $total_active_steps"

    for platform in "${__target_platforms[@]}"; do
      # Split platform into os and arch. The platform string format is os/arch.
      local os="${platform%%/*}"
      local arch="${platform##*/}"
      # Currently only linux images are built in the "compile" action, see
      # https://github.com/kubernetes/kubernetes/blob/1a983bb958ba663e5d86983c55a187fb8c429c51/build/lib/release.sh#L384
      if [[ "${os}"  != "linux" ]]; then
        log.info "skipping non-linux images"
        continue
      fi

      # Load docker image from tarball.
      tarball_path="${KUBE_ROOT}/_output/release-images/${arch}/${image_shortname}.tar"

      # After this, the local Docker daemon gets populated with all image tags
      # (names) stipulated inside the image tarball's manifest.json's `RepoTags'
      # array.
      docker load -i "${tarball_path}"

      # Get unique image name from within the tarball. This is the very first
      # image to use as the argument for the "FROM" directive in a Dockerfile. We
      # pick out the very first name in `RepoTags' because it doesn't matter which
      # one we choose (and because this array might only have 1 element in it).
      log.info "reading tarred manifest.json at ${tarball_path}"
      FROM_image=$(tar xf "${tarball_path}" manifest.json -O \
        | _yq read --stripComments - '[0].RepoTags[0]')

      # If there are no actions other than just the initial "from:" action, save
      # it as-is. This is essentially an empty action, and serves to save the
      # image back into a different tarball path or a new docker image name in the
      # local Docker daemon.
      if (( total_active_steps == 0 )); then
        tag_output_images "package.gcr.images[$i].default-step" "${FROM_image}" "${image_shortname}" "${os}" "${arch}"
      fi

      # Process all actions to be performed for this image.
      for ((j=0; j<total_steps; ++j)); do
        log.info "running step $((j+1))/$total_steps"
        action=$(get_val "package.gcr.images[$i].build-steps[$j].action")
        # Perform the action. Use the resulting name to overwrite
        # FROM_image, so that we can re-use it in the next iteration as
        # the basis for its FROM image.
        #
        # Supported actions are:
        #
        #   - inject_licenses
        #   - inject_source_code
        #
        if [[ "${to_skip[@]}" =~ $j ]]; then
          log.info "skipping step $((j+1)) (${action})"
          continue
        fi
        ${action//-/_} "package.gcr.images[$i].build-steps[$j]" "${FROM_image}"  "${image_shortname}" "${os}" "${arch}"
      done
    done
  done

  log.info "finished building all Docker images"
}

inject_licenses()
{
  local config_key
  local FROM_image
  local image_shortname
  local os
  local arch
  local outputs
  local licenses_path
  local licenses_path_candidates
  local tmp_dir
  config_key="${1:-}"
  FROM_image="${2:-}"
  image_shortname="${3:-}"
  os="${4:-}"
  arch="${5:-}"

  if [[ -z "${config_key}" ]]; then
    log.fail "\`\$config_key' cannot be empty"
  fi

  if [[ -z "${FROM_image}" ]]; then
    log.fail "\`\$FROM_image' cannot be empty"
  fi

  if [[ -z "${image_shortname}" ]]; then
    log.fail "\`\$image_shortname' cannot be empty"
  fi

  if [[ -z "${os}" ]]; then
    log.fail "\`\$os' cannot be empty"
  fi

  if [[ -z "${arch}" ]]; then
    log.fail "\`\$arch' cannot be empty"
  fi

  log.info "appending license layer to ${FROM_image}"
  __config_val=$(get_val "${config_key}")
  log.debugvar __config_val
  log.debugvar FROM_image

  # Determine where the licenses are. This can be either a file or directory.
  # The candidates near the bottom are preferred over those at the top.
  licenses_path_candidates=(
    "${KUBE_ROOT}/Godeps/LICENSES"
    "${KUBE_ROOT}/LICENSES"
  )
  for licenses_path_candidate in "${licenses_path_candidates[@]}"; do
    [[ -e "${licenses_path_candidate}" ]] && licenses_path="${licenses_path_candidate}"
  done
  if [[ -z "${licenses_path}" ]]; then
    log.fail "could not find licenses"
  fi

  tmp_dir="$(mktemp -d -t gke-build-inject-licenses.XXXXXXXXXX)"

  # Inject license by building a new image that wraps the old one. Choice of
  # "THIRD_PARTY_NOTICES" comes from
  # http://go/ospo-cloud-image-faq#8-how-should-the-third-party-component-licenses-and-copyright-notices-be-surfaced.
  cat <<EOF >"${tmp_dir}"/Dockerfile
FROM ${FROM_image}
COPY $(basename "${licenses_path}") /THIRD_PARTY_NOTICES/
EOF

  cp -a "${licenses_path}" "${tmp_dir}"

  log.info "running \`docker build' in ${tmp_dir}"

  # By reusing "$FROM_image", we replace the original. More precisely, the
  # local Docker daemon moves the tag to this new image, leaving the old
  # image tagless if no other tags reference it.
  #
  # NOTE: Building from a FROM_image that references a *.gcr.io registry
  # results in gcloud checking for login credentials (presumably in case the
  # FROM is referencing a private GCR). So, it helps to be logged in before
  # running this script.
  docker build "${tmp_dir}" \
    -t "${FROM_image}" \
    --label "INCLUDES_NOTICES=/THIRD_PARTY_NOTICES"

  tag_output_images "${config_key}" "${FROM_image}" "${image_shortname}" "${os}" "${arch}"

  rm -rf "${tmp_dir}"
}

inject_source_code()
{
  local config_key
  # shellcheck disable=SC2034
  local config_val
  local FROM_image
  local image_shortname
  local os
  local arch
  local archive_name
  local total_git_archive_ignore_paths
  local tmp_dir
  local i
  config_key="${1:-}"
  FROM_image="${2:-}"
  image_shortname="${3:-}"
  os="${4:-}"
  arch="${5:-}"

  if [[ -z "${config_key}" ]]; then
    log.fail "\`\$config_key' cannot be empty"
  fi

  if [[ -z "${FROM_image}" ]]; then
    log.fail "\`\$FROM_image' cannot be empty"
  fi

  if [[ -z "${image_shortname}" ]]; then
    log.fail "\`\$image_shortname' cannot be empty"
  fi

  if [[ -z "${os}" ]]; then
    log.fail "\`\$os' cannot be empty"
  fi

  if [[ -z "${arch}" ]]; then
    log.fail "\`\$arch' cannot be empty"
  fi

  # shellcheck disable=SC2034
  __config_val=$(get_val "${config_key}")
  log.debugvar __config_val
  log.debugvar FROM_image

  archive_name=$(get_val "${config_key}.params.archive-name")

  if [[ -z "${archive_name}" ]]; then
    log.fail "\`\$archive_name' cannot be empty"
  fi

  log.info "appending source code layer to ${FROM_image}"

  total_git_archive_ignore_paths=$(get_val "${config_key}.params.git-archive-ignore-paths" -l)
  for ((i=0; i<total_git_archive_ignore_paths; ++i)); do
    echo "$(get_val "${config_key}.params.git-archive-ignore-paths[$i]") export-ignore" >> "${KUBE_ROOT}"/.gitattributes
  done
  # Package up source code with git-archive(1).
  tmp_dir="$(mktemp -d -t gke-build-inject-source-code.XXXXXXXXXX)"
  git -C "${KUBE_ROOT}" archive \
    --worktree-attributes \
    --format=tar \
    HEAD | xz > "${tmp_dir}/${archive_name}"

  echo "FROM ${FROM_image}" > "${tmp_dir}"/Dockerfile
  echo "COPY ${archive_name} /" >> "${tmp_dir}"/Dockerfile

  # Reset .gitattributes to keep the git working tree clean.
  git checkout "${KUBE_ROOT}"/.gitattributes

  # The "SOURCES_INCLUDED" field is fairly meaningless here as it is just a
  # period (used to denote "all sources in the Kubernetes project root").
  docker build "${tmp_dir}" \
    -t "${FROM_image}" \
    --label "INCLUDES_SOURCE=/${archive_name}" \
    --label "SOURCES_INCLUDED=."

  tag_output_images "${config_key}" "${FROM_image}" "${image_shortname}" "${os}" "${arch}"

  rm -rf "${tmp_dir}"
}

# Tags FROM_image to multiple GCR repos to prepare for final push.
tag_output_images()
{
  local config_key
  local FROM_image
  local image_shortname
  local os
  local arch
  local tag
  local outputs
  local output_image
  local tarball_path
  local i
  local local_tarball

  config_key="${1:-}"
  FROM_image="${2:-}"
  image_shortname="${3:-}"
  os="${4:-}"
  arch="${5:-}"

  if [[ -z "${config_key}" ]]; then
    log.fail "\`\$config_key' cannot be empty"
  fi

  if [[ -z "${FROM_image}" ]]; then
    log.fail "\`\$FROM_image' cannot be empty"
  fi

  if [[ -z "${image_shortname}" ]]; then
    log.fail "\`\$image_shortname' cannot be empty"
  fi

  if [[ -z "${os}" ]]; then
    log.fail "\`\$os' cannot be empty"
  fi

  if [[ -z "${arch}" ]]; then
    log.fail "\`\$arch' cannot be empty"
  fi

  # Retrieve the Docker tag (everything after the colon ':') from the
  # FROM_image.
  if [[ ! "${FROM_image}" =~ .+:.+ ]]; then
    log.fail "\`\$FROM_image' must be in the form <IMAGE>:<TAG>"
  fi
  tag="${FROM_image#*:}"

  log.info "saving image ${FROM_image} into outputs"

  # Tag image with one or more output names.
  outputs=$(get_val "${config_key}.outputs" -l)
  if [[ -n "${outputs}" ]]; then
    for ((i=0; i<$(get_val "${config_key}.outputs" -l); ++i)); do
      output_gcr_repo="$(get_val "${config_key}.outputs[$i]")"
      # Currently windows images are not pushed to GCR and ${os} is not included
      # in gcr repo path. See https://github.com/kubernetes/kubernetes/blob/1a983bb958ba663e5d86983c55a187fb8c429c51/build/lib/release.sh#L384
      output_image="${output_gcr_repo}/${image_shortname}-${arch}"
      log.info "saving to ${output_image}:${tag}"
      docker tag "${FROM_image}" "${output_image}:${tag}"
    done
  fi
  # If local_tarball is true, also save image locally as tarball.
  local_tarball=$(get_val "${config_key}.local_tarball")
  if [[ "${local_tarball}" == "true" ]]; then
    tarball_path="${KUBE_ROOT}/_output/release-stage/server/${os}-${arch}/kubernetes/server/bin/${image_shortname}.tar"
    log.info "saving image to ${tarball_path}"
    docker save -o "${tarball_path}" "${FROM_image}"
  fi
}

# Arrange already-built files in a particular way in preparation for upload to
# GCS. Also, add checksums.
arrange_for_gcs()
{
  local for_gcs_tmp
  local platform
  local platform_src

  mkdir -p "${__for_gcs}"

  # NOTE: "release-images", ""release-stage", and "release-tars" directory
  # names are hardcoded outside by the upstream build system, in
  # package-tarballs.sh.
  local release_stage_dir="${KUBE_ROOT}/_output/release-stage"
  local release_tars_dir="${KUBE_ROOT}/_output/release-tars"
  local gci_path=$release_stage_dir/full/kubernetes/cluster/gce/gci
  declare -a srcs=()
  local dst

  log.info "locally stage release artifacts..."

  # Copy GCS artifacts into a temporary folder. This way, we don't have to run
  # 'rm -rf ${__for_gcs}' which can be a security risk if ${__for_gcs} is
  # accidentally set to '/'.
  for_gcs_tmp="$(mktemp -d -t gke-build-for-gcs.XXXXXXXXXX)/${KUBE_GIT_VERSION}"
  # Create directories in preparation for copying into them.
  mkdir -p "${for_gcs_tmp}/extra/gce/windows"
  # Create a symlink to the temp dir to make it easier for devs to browse it.
  ln -sfv "${for_gcs_tmp}" "${__for_gcs}"

  # Stage everything in release directory
  log.info "Preparing for GCS upload into ${__for_gcs}/${KUBE_GIT_VERSION}..."

  # Copy all tarballs in release-tars/.
  cp -fv "${release_tars_dir}"/* "${__for_gcs}/${KUBE_GIT_VERSION}"

  cp -fv \
    "${gci_path}"/node.yaml \
    "${gci_path}"/master.yaml \
    "${gci_path}"/configure.sh \
    "${gci_path}"/shutdown.sh \
    "${__for_gcs}/${KUBE_GIT_VERSION}"/extra/gce

  # Having the Windows startup scripts from the GCE cluster deploy hosted with
  # the release is useful for GKE.
  windows_local_path=$release_stage_dir/full/kubernetes/cluster/gce/windows
  if [[ -d "${windows_local_path}" ]]; then
    cp -fv \
      "$windows_local_path"/configure.ps1 \
      "$windows_local_path"/common.psm1 \
      "$windows_local_path"/k8s-node-setup.psm1 \
      "$windows_local_path"/testonly/install-ssh.psm1 \
      "$windows_local_path"/testonly/user-profile.psm1 \
      "${__for_gcs}/${KUBE_GIT_VERSION}"/extra/gce/windows
  fi

  # Upload the "naked" binaries to GCS. This is useful for install scripts
  # that download the binaries directly and don't need tars.
  for platform in "${__target_platforms[@]}"; do
    platform_src="${platform/\//-}"
    dst="${__for_gcs}/${KUBE_GIT_VERSION}/bin/${platform}"
    mkdir -p "${dst}"

    log.info "Copying in raw binaries for ${platform}..."

    # Try to copy in all server binaries, if they exist.
    if [[ -d "$release_stage_dir/server/$platform_src/kubernetes/server" ]]; then
      srcs=("$release_stage_dir/server/$platform_src/kubernetes/server/bin/"*)
      cp -fv "${srcs[@]}" "$dst"
    fi
    # Try to copy in all client binaries, if they exist.
    if [[ -d "$release_stage_dir/client/$platform_src/kubernetes/client" ]]; then
      srcs=("$release_stage_dir/client/$platform_src/kubernetes/client/bin/"*)
      cp -fv "${srcs[@]}" "$dst"
    fi

    # Copy in node binaries if they exist and this isn't a 'server' platform.
    if [[ ! -d "$release_stage_dir/server/$platform_src" ]]; then
      if [[ -d "$release_stage_dir/node/$platform_src" ]]; then
        srcs=("$release_stage_dir/node/$platform_src/kubernetes/node/bin/"*)
        cp -fv "${srcs[@]}" "$dst"
      fi
    fi
  done

  write_checksums "${__for_gcs}/${KUBE_GIT_VERSION}"
}

write_checksums()
{
  local toplevel="${1}"

  log.info "Writing checksums..."

  # Write the release checksum files. The trailing >/dev/null is to hide the
  # results of 'find' from stdout.
  checksum_md5="$(mktemp)"
  checksum_sha1="$(mktemp)"
  checksum_sha256="$(mktemp)"
  checksum_sha512="$(mktemp)"
  # This pushd is for setting the starting directory for xargs.
  pushd "${toplevel}"

  # We use "-printf '%P\0'" instead of "-print0" because the former hides the
  # unnecessary leading "./" prefix.
  find . -type f -printf '%P\0' \
    | sort -z \
    | tee \
      >(xargs -0 md5sum  > "${checksum_md5}") \
      >(xargs -0 sha1sum   > "${checksum_sha1}") \
      >(xargs -0 sha256sum > "${checksum_sha256}") \
      >(xargs -0 sha512sum > "${checksum_sha512}") \
      >/dev/null

  # Save individual checksums for each file. These are used by downstream
  # scripts, including
  # google3/cloud/kubernetes/engine/tools/preload_image/preloader_script.sh.template.
  find . -type f | while read -r path; do
    get_checksum "${path}" md5  > "${path}.md5"
    get_checksum "${path}" sha1   > "${path}.sha1"
    get_checksum "${path}" sha256 > "${path}.sha256"
    get_checksum "${path}" sha512 > "${path}.sha512"
  done

  # The toplevel checksums were written outside the directory being searched
  # (so that these files are not themselves contained inside the checksums),
  # so move them back into "${__for_gcs}".
  mv "${checksum_md5}"  "${toplevel}/MD5SUMS"
  mv "${checksum_sha1}"   "${toplevel}/SHA1SUMS"
  mv "${checksum_sha256}" "${toplevel}/SHA256SUMS"
  mv "${checksum_sha512}" "${toplevel}/SHA512SUMS"
  popd

  # For debugging, print MD5SUMS contents for a quick look at each substantive
  # file that is part of the release.
  cat "${toplevel}/MD5SUMS"
}

get_checksum()
{
  local file=$1
  local algo=$2
  case $algo in
    md5|sha1|sha256|sha512)
      "${algo}sum" "$file" | cut -d' ' -f1 ;;
    *)
      log.fail "unsupported algorithm \`${algo}'" ;;
  esac
}

# Push GCR artifacts. Additionally, use the `docker manifest' command to
# generate fat manifests of the just-pushed images. The fat manifest generation
# occurs at the end, because Docker requires that the images exist first in a
# Docker registry (in this case, GCR).
push_gcr()
{
  local child_image
  local manifest_list
  local container_registry
  local image_shortname
  local os
  local arch
  local i
  local j

  log.debugvar __docker_tag

  # The basic idea is (1) generate cross-product of desired push repos and
  # images, and then (2) docker-push them. Because we expect to only run this
  # code for pushing images from within Louhi where the images should already
  # be populated, we don't have to bother "docker load"-ing them into the
  # local Docker daemon (the image should already be there).
  for ((i=0; i<$(get_val "push.gcr.repos" -l); ++i)); do
    container_registry="$(get_val "push.gcr.repos[$i]")"
    log.debugvar container_registry
    for ((j=0; j<$(get_val "push.gcr.images" -l); ++j)); do
      image_shortname="$(get_val "push.gcr.images[$j]")"
      log.debugvar image_shortname
      for platform in "${__target_platforms[@]}"; do
        # Split platform into os and arch. The platform string format is os/arch.
        os="${platform%%/*}"
        arch="${platform##*/}"
        log.debugvar os
        log.debugvar arch
        # Currently only linux images are built in the "compile" action, see
        # https://github.com/kubernetes/kubernetes/blob/1a983bb958ba663e5d86983c55a187fb8c429c51/build/lib/release.sh#L384
        if [[ "${os}"  != "linux" ]]; then
          log.info "skipping non-linux images"
          continue
        fi
        # These are the exact images to push. Failure to push any of these
        # images will result in an overall push failure.
        child_image="${container_registry}/${image_shortname}-${arch}:${__docker_tag}"
        log.debugvar child_image

        # First, push the per-platform child image.
        docker push "${child_image}"
        log.info "done pushing image ${child_image}"

        # Second, create manifest lists to group together all images for all
        # architectures pushed up just above.
        manifest_list="${container_registry}/${image_shortname}"
        # Passing the --amend flag lets us invoke `docker manifest create`
        # multiple times and still modify the same manifest list.
        docker manifest create \
          --amend "${manifest_list}:${__docker_tag}" \
          "${child_image}"
        docker manifest annotate \
          --os "${os}" --arch "${arch}" \
          "${manifest_list}:${__docker_tag}" "${child_image}"
      done

      # Finally, push the manifest list, but delete the local reference to it
      # afterwards. This is mainly for cleanliness in case this script
      # is run again in a new invocation (useful for debugging; NOP
      # for production pushes).
      docker manifest push --purge "${manifest_list}:${__docker_tag}"
      log.info "done pushing docker manifest ${manifest_list}:${__docker_tag}"
    done
  done
}

# Push GCS artifacts.
push_gcs()
{
  local artifact_dir_toplevel
  local artifact_dir
  declare -a gcs_buckets=()
  local gcs_bucket
  local i

  # Get directory to push up.
  artifact_dir_toplevel="$(get_val "push.gcs.from")"
  # Remove and replace literal "${KUBE_ROOT}" with the actual value.
  artifact_dir_toplevel=${artifact_dir_toplevel/\${KUBE_ROOT\}/${KUBE_ROOT}}
  # Set the actual directory to upload.
  artifact_dir="${artifact_dir_toplevel}/${KUBE_GIT_VERSION}"

  log.debugvar artifact_dir

  if [[ ! -d "${artifact_dir}" ]]; then
    log.fail "directory \`${artifact_dir}' does not exist"
  fi

  # If we are manually supplied a bucket, use it instead of using the buckets
  # specified in the config yaml.
  if [[ -n "${__GCS_BUCKET:-}" ]]; then
    gcs_buckets+=("${__GCS_BUCKET}")
  else
    for ((i=0; i<$(get_val "push.gcs.to" -l); ++i)); do
      gcs_buckets+=("$(get_val "push.gcs.to[$i]")")
    done
  fi

  for gcs_bucket in "${gcs_buckets[@]}"; do
    push_gcs_with_checksum "${artifact_dir}" "${gcs_bucket}"

    # List all files up in the bucket as a validity check. The trailing "**"
    # literal wildcard is gsutil syntax for a flat listing of all objects in
    # the trailing subdirectory (in this case, "${version_dir}").
    gsutil ls -l "${gcs_bucket}/${KUBE_GIT_VERSION}/**"
  done
}

# Check the MD5 checksums of objects in the GCS bucket against what we
# have locally. List those objects that either are not in the GCS bucket
# or whose checksums don't match with what we have locally. In other
# words, this shows you only those local objects that must get uploaded
# into the bucket. For each object, the output format is "OBJ,HASH".
get_unreconciled_artifacts()
{
  local artifact_dir="${1:-}"
  local gcs_bucket="${2:-}"
  local artifact_dir_toplevel
  local version_dir
  declare -a artifacts=()
  local artifact
  local checksum_str
  local checksum_base64
  declare -A remote_checksum

  log.assertvar artifact_dir
  log.assertvar gcs_bucket

  artifact_dir_toplevel="$(dirname "${artifact_dir}")"
  version_dir="$(basename "${artifact_dir}")"

  for objhash in $(gsutil ls -L "${gcs_bucket}/${version_dir}/**" \
    | grep '^gs\|^  Hash (md5)' \
    | paste -d, - - \
    | sed 's/:,  Hash (md5): \+/,/'); do
    remote_checksum["${objhash%,*}"]="${objhash#*,}"
  done

  mapfile -t artifacts < <(cd "${artifact_dir_toplevel}" && find -L "${version_dir}" -type f | sort)

  for artifact in ${artifacts[@]+"${artifacts[@]}"}; do
    checksum_str=$(md5sum "${artifact_dir_toplevel}/${artifact}" | cut -d' ' -f1)
    checksum_base64="$(hex_encode "${checksum_str}" | base64)"
    if [[ -v remote_checksum["${gcs_bucket}/${artifact}"] && "${checksum_base64}" == "${remote_checksum[${gcs_bucket}/${artifact}]}" ]]; then
      continue
    fi
    echo "${artifact},${checksum_base64}"
  done
}

# Pushes up artifact_dir into gcs_bucket. The name of the artifact_dir itself
# will be in the bucket. See
# https://cloud.google.com/storage/docs/gsutil/addlhelp/HowSubdirectoriesWork.
push_gcs_with_checksum()
{
  local artifact_dir="${1:-}"
  local gcs_bucket="${2:-}"
  declare -a unreconciled_artifacts=()
  local artifact_dir_toplevel
  local version_dir
  declare -a artifacts=()
  local artifact
  local checksum_base64
  local artifacthash

  log.assertvar artifact_dir
  log.assertvar gcs_bucket

  artifact_dir_toplevel="$(dirname "${artifact_dir}")"
  version_dir="$(basename "${artifact_dir}")"

  mapfile -t unreconciled_artifacts < <(get_unreconciled_artifacts "${artifact_dir}" "${gcs_bucket}")

  if (( "${#unreconciled_artifacts[@]}" == 0 )); then
    log.debug "nothing new to push"
    return
  fi

  for artifacthash in ${unreconciled_artifacts[@]+"${unreconciled_artifacts[@]}"}; do
    # Calculate md5 checksum of each artifact. The checksum is used
    # for each upload as per
    # https://cloud.google.com/storage/docs/gsutil/commands/cp#checksum-validation.
    # We use md5sum because it's faster than the Python
    # implementation of "gsutil hash".
    #
    # NOTE: This for-loop is painfully slow, but it has all of
    # the advantages of safety listed in the link above because
    # it guarantees that GCS will never have
    # bad-looking/corrupted objects.
    artifact="${artifacthash%,*}"
    checksum_base64="${artifacthash#*,}"
    gsutil \
      -q \
      -h "Content-MD5:${checksum_base64}" \
      cp \
      "${artifact_dir_toplevel}"/"${artifact}" \
      "${gcs_bucket}/${artifact}"
    # Print '.' symbol to denote progress without spamming the
    # log output.
    echo -n .
  done
  echo
}

# Get each octet (in hex) in the input string and prepend a "\x" literal to it
# into the output. It is assumed that the input string is a hex string and that
# it has an even number of nibbles (that is, 8*n bits where n>0).
#
# Example:
#  INPUT: "abcd"
#   OUTPUT: "\xab\xcd" (not literally this string, but in raw bytes)
hex_encode()
{
  local checksum_str="${1:-}"
  local output=""
  local i
  log.assertvar checksum_str

  for ((i=0; i<${#checksum_str}/2; ++i)); do
    output+="\\x${checksum_str:$((i*2)):2}"
  done

  echo -en "${output}"
}

# Finds Docker images in the local Docker daemon. The name_regex argument ($1)
# must be the full name of the image (until the colon ':' character), and not
# just a prefix.
find_images()
{
  local name_regex
  local tag
  declare -a found_images_candidates=()
  declare -a found_images=()

  name_regex="$1"
  tag="${2:-}"

  mapfile -t found_images_candidates < <(docker images --format "{{.Repository}}:{{.Tag}}" \
    | grep -v '<none>' | grep "^${name_regex}:[^:]\+$")

  # If we specified a tag, then use it to further filter only those images
  # that match the tag in addition to the image name.
  if [[ -n "${tag}" ]]; then
    # The ${arr[@]+"${arr[@]}"} idiom is to support Bash 4.3 and older
    # versions which emit an "unbound variable" error for expansions on the
    # empty array. See
    # https://stackoverflow.com/questions/7577052/bash-empty-array-expansion-with-set-u
    # and https://gist.github.com/dimo414/2fb052d230654cc0c25e9e41a9651ebe.
    for candidate in ${found_images_candidates[@]+"${found_images_candidates[@]}"}; do
      if [[ "${candidate#*:}" == "${tag}" ]]; then
        found_images+=("${candidate}")
      fi
    done
  else
    found_images=("${found_images_candidates[@]}")
  fi

  if (( ${#found_images[@]} == 0 )); then
    log.warn "no images found for regex \`${name_regex}' and tag \`${tag}'"
    return 1
  fi
  echo "${found_images[@]}"
}

# Remove all entries in the local (on-disk) toplevel artifact folder --- both
# the symlinks and also the actual folders holding the artifacts are deleted.
clean()
{
  local artifact_dir_toplevel

  artifact_dir_toplevel="$(get_val "push.gcs.from")"

  # The argument "bash" is provided to be $0 for the internal script. Although
  # it is not used in the internal script, it will be used if there is an
  # error message encountered.
  find "${artifact_dir_toplevel}" \
    -type l \
    -exec bash -c 'rm -rfv "$(readlink -f "$1")" $1' bash {} \;
}

get_val()
{
  # If the __GKE_BUILD_CONFIGS array exists, merge the values inside
  # the YAMLs together (later YAML files gaining prececedence over the
  # previous ones).
  if [[ -v __GKE_BUILD_CONFIGS ]]; then
    _yq merge --arrays=overwrite --overwrite "${__GKE_BUILD_CONFIGS[@]}" \
      | _yq read --stripComments - "$@"
  else
    _yq read --stripComments "${__GKE_BUILD_CONFIG}" "$@"
  fi
}

# Check that some invariants are observed.
self_test()
{
  local got
  local expected

  log.info "BEGIN self_test"

  # If the version number already has a gke dev version in it, NOP.
  got=$(inject_gke_dev_version_marker "v1.20.0-gke.1200.99+0-g0123456789abcd")
  expected="v1.20.0-gke.1200.99+0-g0123456789abcd"
  assert_variable_equality "${got}" "${expected}"

  # If there is a "-gke.N", preserve this "N" when injecting the rest of the dev
  # version marker.
  got=$(inject_gke_dev_version_marker "v1.20.2-gke.800-1-g8783f10452667a")
  expected="v1.20.2-gke.800.99+1-g8783f10452667a"
  assert_variable_equality "${got}" "${expected}"

  # If there is no "-gke.N" pattern at all, inject "-gke.99.99+".
  got=$(inject_gke_dev_version_marker "v1.21.0-alpha.0-692-g94a623a45a77eb")
  expected="v1.21.0-gke.99.99+alpha.0-692-g94a623a45a77eb"
  assert_variable_equality "${got}" "${expected}"

  # If we're forced to inject a dev version marker into a plain tag without any
  # "-gke.N", still append the "-gke.99.99+" substring.
  got=$(inject_gke_dev_version_marker "v1.21.0")
  expected="v1.21.0-gke.99.99+dev-unknown"
  assert_variable_equality "${got}" "${expected}"

  # If we're given a prod version "v1.21.0-gke.1200" *without* any other
  # information, we need to (1) preserve the "1200" in "gke.1200", but also
  # append ".99". However we also need to append a "+dev-unknown" because that's
  # what
  # https://source.corp.google.com/piper///depot/google3/cloud/kubernetes/engine/common/version.go;l=160
  # expects (that there exist some "+..." string after the "-gke.N.N").
  got=$(inject_gke_dev_version_marker "v1.21.0-gke.1200")
  expected="v1.21.0-gke.1200.99+dev-unknown"
  assert_variable_equality "${got}" "${expected}"

  # Already has a + sign.
  got=$(inject_gke_dev_version_marker "v1.21.0-gke.1200+stuff")
  expected="v1.21.0-gke.1200.99+stuff"
  assert_variable_equality "${got}" "${expected}"

  log.info "END self_test (OK)"
}

# Git branch-specific tweaks for various settings. This function is expected to
# be modified whenever there are any special cases we need to cover based on the
# branch.
branch_hook()
{
  local branch="${1:-}"

  # INJECT_DEV_VERSION_MARKER by branch.
  case "${branch}" in
      release-*-gke.*)
        # For release branches, do not inject the dev marker. This is because we
        # are presumably running this build for an actual production release.
        log.info "on a release branch, disabling __INJECT_DEV_VERSION_MARKER"
        __INJECT_DEV_VERSION_MARKER=0 ;;
      *)
        # For all non-release branches, inject a dev marker, because there might
        # not be a Git tag nearby such that we can auto-generate a GKE
        # release-like version string of the form `vn.n.n-gke.n` (see
        # set_version() in lib_gke.sh).
        __INJECT_DEV_VERSION_MARKER=1 ;;
  esac

}
